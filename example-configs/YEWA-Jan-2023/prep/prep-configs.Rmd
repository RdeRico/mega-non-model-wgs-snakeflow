---
title: "Prepping the Configs for YEWA"
output: html_notebook
---

This is going to happen in:
```
/home/eanderson/scratch/PROJECTS/YEWA-Jan-2023
```
First thing I do is clone the mega-non-model repo:
```sh
git clone git@github.com:eriqande/mega-non-model-wgs-snakeflow.git
cd mega-non-model-wgs-snakeflow/
```

From here on out, everything that I do is done with this:
```sh
/home/eanderson/scratch/PROJECTS/YEWA-Jan-2023/mega-non-model-wgs-snakeflow
```
as the working directory (or "top-level" directory).


## Downloading the fastqs

Now I am going to download all the different files from Sharepoint into the
directory `data`, using rclone.

First we check with a dry-run:
```sh
rclone copy --dry-run BGP-sharepoint:Genetic_and_Environmental_Data/Novoseq_fastq_2_download data \
    --include='YEWA_redo_Novoseq_01032023/**' \
    --include='YEWA_transfer/YEWA_LCWG_Plate1_11032021/**' \
    --include='YEWA_transfer/YEWA_LCWG_Plate2_03212022_03312022/Rayne_7572_220328A7/**'
```
Which tells us, in summary, that this will do:
```
Transferred:   	  982.207 GiB / 982.207 GiB, 100%, 40.710 GiB/s, ETA 0s
Transferred:         1727 / 1727, 100%
Elapsed time:        28.1s
2023/01/15 14:45:21 NOTICE:
Transferred:   	  982.207 GiB / 982.207 GiB, 100%, 40.710 GiB/s, ETA 0s
Transferred:         1727 / 1727, 100%
Elapsed time:        28.1s
```
So, about a Tb of data.  And it looks like it has the three main directories worth 
of data.  So, we will go ahead and launch this:
```sh
rclone copy BGP-sharepoint:Genetic_and_Environmental_Data/Novoseq_fastq_2_download data     --include='YEWA_redo_Novoseq_01032023/**'     --include='YEWA_transfer/YEWA_LCWG_Plate1_11032021/**'     --include='YEWA_transfer/YEWA_LCWG_Plate2_03212022_03312022/Rayne_7572_220328A7/**'
```
This is coming down at 6 to 7 Mb/sec.  A lot slower than we often saw with Google Drive.
It is estimated to take about 22 hours to get that done.  OK.  But then it started
saying about two days, with download speeds of 5 Mb.  Seriously throttled.

Maybe I need to register my own ID with sharepoint.

So, I killed the transfer and now I am following the directions
at: https://rclone.org/onedrive/#creating-client-id-for-onedrive-personal


I have an app named `New Rclone` that has this as the "application (client) ID"
2a6f216e-16ca-480f-80ee-4399895362a2

My rclone secret has the "value" CRA8Q************************** (censored)

And the secret ID is: 57edd5ef-**************************** (censored)

My tenant ID is: afb58802-ff7a-4bb1-ab21-367ff2ecfc8b

So my auth_url will be:

https://login.microsoftonline.com/afb58802-ff7a-4bb1-ab21-367ff2ecfc8b/oauth2/v2.0/authorize

and my token URL will be:

https://login.microsoftonline.com/afb58802-ff7a-4bb1-ab21-367ff2ecfc8b/oauth2/v2.0/token

This was a huge nightmare because rclone couldn't handle the encoded token that came
back.  I ended up putting the encoded token into the website at https://jwt.io/
and decoding it into a token, which I then edited with nano into the rclone config
file, and I also had to add the drive_id and drive_type into there, too.

With that I was finally able to access the SharePoint with my remote: `BGP-SharePoint-ID`.

I will do a quick test to see if it is any faster now.

No change.  I also tried adding a --user-agent option:
```sh
rclone copy --user-agent "ISV|rclone.org|rclone/v1.61.1"   BGP-SharePoint-ID:Genetic_and_Environmental_Data/Novoseq_fastq_2_download data     --include='YEWA_redo_Novoseq_01032023/**'     --include='YEWA_transfer/YEWA_LCWG_Plate1_11032021/**'     --include='YEWA_transfer/YEWA_LCWG_Plate2_03212022_03312022/Rayne_7572_220328A7/**'
```
All I am getting is 6.4 Mb/sec.  Still gonna take about two days...

## Making the units file

Once all the downloads were complete, I listed all the non-undetermined fastqs that
we will be dealing with in this way:
```sh
ls -l  data/*/*fastq.gz data/YEWA_transfer/YEWA_LCWG_Plate1_11032021/raw_data/*/*.fq.gz data/YEWA_transfer/YEWA_LCWG_Plate2_03212022_03312022/Rayne_7572_220328A7/*.fastq.gz | awk '!/Undetermined/'
```
and I put those into the example-configs in:
```
example-configs/YEWA-Jan-2023/prep/inputs/fq-listsings.txt
```

And now we can start processing that stuff  Since the file names
don't all follow the same conventions, we have to do slightly different
things with the `.fq.gz` ones versus the `.fastq.gz` ones.  We throw in
a little `case_when()` for those situations.
```{r, message=FALSE, warning=FALSE}
library(tidyverse)

files <- read_table("inputs/fq-listsings.txt", col_names = FALSE) %>%
  select(X9, X5) %>%
  mutate(kb = X5/1000) %>%
  rename(fq = X9) %>%
  select(fq, kb) %>%
  mutate(base = basename(fq)) %>%
  mutate(
    sample_id = case_when(
      str_detect(base, "\\.fq\\.gz") ~ str_match(base, "^s(.*)_[12]\\.fq\\.gz")[,2],
      str_detect(base, "\\.fastq\\.gz") ~ str_match(base, "^(.*)_S[0-9]+.*\\.fastq\\.gz")[,2],
      TRUE ~ NA_character_
    )
  ) %>%
  mutate(
    read = case_when(
      str_detect(base, "\\.fq\\.gz") ~ str_match(base, "^.*_([12])\\.fq\\.gz")[,2],
      str_detect(base, "\\.fastq\\.gz") ~ str_match(base, "^.*_R([12])_001*\\.fastq\\.gz")[,2],
      TRUE ~ NA_character_
    )
  ) 
```

Now, because the naming of the files does not always have the lane, etc., and
we want to machine name (not really necessary, but we can get it easily from
the first line of each file), I make a file that has that info that we can join
onto the path:
```sh
for i in  data/*/*fastq.gz data/YEWA_transfer/YEWA_LCWG_Plate1_11032021/raw_data/*/*.fq.gz data/YEWA_transfer/YEWA_LCWG_Plate2_03212022_03312022/Rayne_7572_220328A7/*.fastq.gz; do zcat $i | awk -v f=$i 'BEGIN {OFS="\t"} NR==1 {print f, $1; exit}'; done | awk '!/Undetermined/'
```
The results of that have been put into:
```
example-configs/YEWA-Jan-2023/prep/inputs/seq-tags-per-path.tsv
```
And we can make it nice like this:
```{r}
seq_ids <- read_tsv("inputs/seq-tags-per-path.tsv", col_names = c("fq", "id")) %>%
  separate(
    id, 
    into = c("x1", "x2", "flowcell", "lane"), 
    sep = ":", 
    extra = "drop"
  ) %>%
  select(-x1, -x2) %>%
  mutate(platform = "ILLUMINA")
```

And we can now join those and pivot them to get fq1 fq2 kb1 and kb2 all on the same
line, and then assign snakemake sample numbers to them.
```{r}
files_wide <- files %>%
  left_join(seq_ids, by = "fq") %>%
  select(-base) %>%
  pivot_wider(
    values_from = c(fq, kb),
    names_from = read,
    names_sep = ""
  ) %>%
  arrange(sample_id, flowcell, lane) %>%
  mutate(
    sample = sprintf("s%04d", as.integer(factor(sample_id, levels = unique(sample_id)))),
    .before = sample_id
  )
```

We will, at the same time, add libraries to them.  It looks like birds were either
done on "Plate1" or "Plate2", and the reseqs were done on a new library prep that
we will call Plate3.
Let us confirm that.  We see if anyone was done on both Plate1 and Plate2:
```{r}
check_plates <- files_wide %>%
  mutate(
    plate = case_when(
      str_detect(fq1, "Plate1") ~ 1L,
      str_detect(fq1, "Plate2") ~ 2L,
      TRUE ~ NA_integer_
    ),
    .after = sample
  ) %>%
  group_by(sample, sample_id) %>%
  summarise(plate_str = paste(plate, collapse = ",")) %>%
  ungroup()
```

And now we can count the configurations:
```{r}
check_plates %>%
  count(plate_str)
```

Kristen thought that there might be 10 new birds on there.  I have an email
in to CH and Marina for confirmation on that (I spoke to Marina and, yes, they
put 10 new birds in there)

At any rate, our different libraries will be Plate1, Plate2,
and Plate3.  We make a data frame to join:
```{r}
lib_tib <- check_plates %>%
  select(sample, plate_str) %>%
  mutate(
    library = case_when(
      plate_str == "1" ~ "Plate1",
      plate_str %in% c("2,2", "2,2,NA") ~ "Plate2",
      plate_str == "NA" ~ "Plate3",
      TRUE ~ NA_character_
    )
  )
```

So, now we can make our units file.  For barcodes I am just going to do the
sample_id + library.  It just needs to be unique for those.
```{r}
units_all <- files_wide %>%
  mutate(
    library = case_when(
      str_detect(fq1, "Plate1") ~ "Plate1",
      str_detect(fq1, "Plate2") ~ "Plate2",
      TRUE ~ "Plate3"
    ),
    .after = sample
  ) %>%
  group_by(sample) %>%
  mutate(unit = 1:n(), .after = sample) %>%
  ungroup() %>%
  mutate(
    barcode = str_c(sample, sample_id, library, sep = "-")
  ) %>%
  select(sample, unit, library, flowcell, platform, lane, sample_id, barcode, fq1, fq2, kb1, kb2)
```

Now, when we previously called variants in some of these birds, we found one that
took way too long and produced a lot of private variants, that we suspected was
a hybrid or mis-identified individual.  That was s0141 in the previous run of these
things, which corresponded to `s2113ywar`.  So, we are going to remove 2113ywar.

Here is the one that we will be tossing:
```{r}
units_all %>%
  filter(sample_id == "2113ywar")
```

So, let's do that:
```{r}
units <- units_all %>%
  filter(sample_id != "2113ywar")
```
And, finally, we write that out:
```{r}
write_tsv(units, file = "../units.tsv")
```

## Getting the reference genome

Since it is not available publicly, we have to download it ourselves.
I just grab the one that we used last time:
```sh
rclone copy  BGP-SharePoint-ID:Genetic_and_Environmental_Data/Species_genetic_data/YEWA/resources/genome.fasta resources/
```

I also copied the fai file into the `prep/inputs` directory.

## Making chromosomes and scaffold groups

We do this with R.  As always, it is important to look at the format
in `.test/chromosomes.tsv` and `.test/scaffold_groups.tsv` to know the format.
```{r}
# as I did before, we will let anything over 30 Mb be a "chromosome" and then
# we will shoot for scaffold groups < 50 Mb in total.
fai <- read_tsv(
  "inputs/genome.fasta.fai", col_names = c("chrom", "len", "x1", "x2", "x3")) %>%
  select(-starts_with("x")) %>%
  mutate(cumlen = cumsum(len))

# here are the lengths:
fai %>%
  mutate(x = 1:n()) %>%
  ggplot(aes(x=x, y = len)) + geom_col()
```
Proceeding:
```{r}
chromos <- fai %>%
  filter(len >= 3e7) %>%
  rename(num_bases = len) %>%
  select(-cumlen)

write_tsv(chromos, file = "../chromosomes.tsv")


# now, get the scaff groups
scaffs <- fai %>%
  filter(len < 3e7)

bin_length <- 5e07

scaff_groups <- scaffs %>%
  mutate(
    cumul = cumsum(len),
    part = as.integer(cumul / bin_length) + 1L
  ) %>%
  mutate(
    id = sprintf("scaff_group_%03d", part),
    .before = chrom
  ) %>%
  select(-part, -cumlen)

# let's just see the total lengths of those scaff_groups
# and also the number of scaffolds in each
scaff_groups %>%
  group_by(id) %>%
  summarise(
    tot_bp = sum(len),
    num_scaff = n()
  )
```
Good, that is not too many scaff groups, and also not too many scaffolds
per any one group.
```{r}
write_tsv(scaff_groups, file = "../scaffold_groups.tsv")
```


## Setting up the config.yaml file

As always, it is important to start from .test/config/config.yaml, because that
is the most up-to-date.  So I copied that to 
```
example-configs/YEWA-Jan-2023/config.yaml
```
And then I edited it as appropriate.

## Getting the scatters file

I set the scatters to "" in the config like this:
```yaml
scatter_intervals_file: ""
```
And then I committed all the stuff I just created here and pushed it up to the cluster.

